GPU friendly TensorFlow implementation of Hogwild!, the sparse asynchronous optimization algorithm, introduced in <a href=https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf>this paper</a> from Berkeley (Go Bears). Leverages the TF estimator API to build a multilayer perceptron with a tf.nn.embedding_lookup_sparse operation at the front to replace the traditional matrix multiplication.

Inputs to the estimator are 4 dimensional, with the first two columns indexing two sparse matrices, the first of which contains the nonzero indices of the dense input (which are in the third column of our input), and the second of which contains the values at that input (which are in the fourth). This is useful in case there are order of magnitude differences in the number of nonzero elements between samples. Since we're working with dummy data, the minimum and maximum number of elements can be set from the command line with `--min_nnz` and `--max_nnz`, with the actual number drawn uniformly between them.

Worth noting that worker instances take some time to spin up (4-5 s per instance), so to get a good benchmark of the speedup it's best to run for tens of thousands of gradient steps so that all workers can get online before training finishes.

Nothing to build, just uses public tensorflow docker releases (doesn't leverage NGC to make agnosticism to CPU/GPU implementation simpler). All that's needed is to run `./run.sh -h` to get a sense for command line options. All profiling, logging, and model saving are done inside the container unless a local directory is specified in the `./run.sh` call.

Right now GPU offers very little acceleration when compared to a CPU implementation with the same number of worker threads. Basic profiling suggests that this is due to the fact that the mem copy from CPU to GPU washes out the drastically improved speed of the embedding lookup on GPU. It's possible for very deep and wide networks at large batches GPU advantage could start to grow, but larger batches can also have a detrimental effect on convergence speed for asynchronous training. More investigation into this is required.
